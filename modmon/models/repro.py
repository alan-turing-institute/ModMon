import argparse
from ..db.connect import get_session
from ..db.schema import Modelversion, Model
import json
from ..models.run import run_model
from os import listdir, devnull, mkdir
import pandas as pd
import shutil
import subprocess
import tempfile

def catalogue_metrics(path, tmpdirname, dev_null):
    shutil.copyfile(path + "/metrics.csv", tmpdirname + "/results/metrics.csv")

    subprocess.run(["git", "add", "metrics.csv"], check=True, cwd=tmpdirname + "/results", stdout=dev_null, stderr=dev_null)
    try:
        subprocess.run(["git", "commit", "-m", "'add reference result'"], check=True, cwd=tmpdirname, stdout=dev_null, stderr=dev_null)
    except subprocess.CalledProcessError:  # Commit will fail when metrics.csv is added unchanged
        pass

    subprocess.run(["catalogue", "engage",
                    "--input_data", "data",
                    "--code", "code"
                    ], check=True, cwd=tmpdirname, stdout=dev_null, stderr=dev_null)
    subprocess.run(["catalogue", "disengage",
                    "--input_data", "data",
                    "--code", "code",
                    "--output_data", "results"
                    ], check=True, cwd=tmpdirname, stdout=dev_null, stderr=dev_null)


def results_match(tmpdirname):
    """Use repro-catalogue to determine whether the model appraisal system generated
    matching metrics for the model to those supplied as reference metrics by the analyst"""
    # Get the location of JSON files generated by repro-catalogue
    reference_metrics_file, generated_metrics_file = sorted(listdir(tmpdirname + "/catalogue_results"))

    # Compare the hashes for the metrics csv
    with open(tmpdirname + "/catalogue_results/" + reference_metrics_file, "r") as f:
        reference_metrics = json.load(f)
    with open(tmpdirname + "/catalogue_results/" + generated_metrics_file, "r") as f:
        generated_metrics = json.load(f)
    if reference_metrics["output_data"]["results"]["results/metrics.csv"] == generated_metrics["output_data"]["results"]["results/metrics.csv"]:
        return True
    return False


def reference_result_is_reproducible(path, metadata):
    session = get_session()
    dev_null = open(devnull, 'w')

    with tempfile.TemporaryDirectory() as tmpdirname:
        subprocess.run(["git", "init"], check=True, cwd=tmpdirname, stdout=dev_null, stderr=dev_null)

        mkdir(tmpdirname + "/data")
        mkdir(tmpdirname + "/code")
        mkdir(tmpdirname + "/results")

        # Use repro-catalogue with reference metrics supplied by analyst
        catalogue_metrics(path, tmpdirname, dev_null)

        # Get active model version for this model
        modelid = session.query(Model).filter_by(name=metadata["model_name"]).first().modelid
        model_version = session.query(Modelversion).filter_by(modelid=modelid).filter_by(active=True).first()

        # Run the model in reference mode (do not add results to db)
        # This overwrites metrics.csv within the dir supplied to modmon_model_check
        run_model(model_version,
                  metadata["data_window_start"],
                  metadata["data_window_end"],
                  metadata["db_name"],
                  reference=True,
                  session=session)

        # Use repro-catalogue with new metrics just generated
        catalogue_metrics(path, tmpdirname, dev_null)

        match_bool = results_match(tmpdirname)
    return match_bool
